{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "In this walkthrough, we’ll review the correct solution to the activity where you implemented backward elimination as a feature selection method. Backward elimination starts with all available features in a dataset and progressively removes those that are statistically insignificant.\n",
        "\n",
        "This step-by-step guide will help you understand the process of removing irrelevant features and refining your ML model.\n",
        "\n",
        "By the end of this walkthrough, you'll be able to:\n",
        "\n",
        "    Implement backward elimination correctly: follow the step-by-step process of applying backward elimination, including adding a constant, fitting the model, and iteratively removing statistically insignificant features.\n",
        "\n",
        "    Interpret model summary: analyze the p-values and coefficients from the model summary to determine which features are significant predictors.\n",
        "\n",
        "    Refine and simplify models: understand how removing irrelevant features can lead to a more efficient and interpretable model, reducing the risk of overfitting."
      ],
      "metadata": {
        "id": "q1VaBfwhc7XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load and prepare the data\n",
        "\n",
        "The first step was to load a sample dataset and prepare the feature matrix (X) and the target variable (y). For this activity, we used a simple dataset of study hours, previous exam scores, and whether the student passed or failed."
      ],
      "metadata": {
        "id": "eMnA9dnhdCRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0qkrtnqJZhG0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'StudyHours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'PrevExamScore': [30, 40, 45, 50, 60, 65, 70, 75, 80, 85],\n",
        "    'Pass': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['StudyHours', 'PrevExamScore']]\n",
        "y = df['Pass']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    StudyHours and PrevExamScore were the input features.\n",
        "\n",
        "    Pass (0 = Fail, 1 = Pass) was the target variable."
      ],
      "metadata": {
        "id": "YWgB6yWkdKKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Add a constant to the model\n",
        "\n",
        "Before performing backward elimination, we added a constant (intercept) to the feature matrix. This is necessary for fitting the model with statsmodels."
      ],
      "metadata": {
        "id": "mw3jXULndMpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Add a constant (for the intercept)\n",
        "X = sm.add_constant(X)"
      ],
      "metadata": {
        "id": "OOMFoA_wdnHq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Fit the initial model\n",
        "\n",
        "The next step was to fit a model using ordinary least squares (OLS) regression, including all the features in the dataset. This initial model serves as the starting point for backward elimination."
      ],
      "metadata": {
        "id": "6MOvfkP6dpU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model using OLS regression\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Display the model summary (including p-values)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7dJgSQrd2RK",
        "outputId": "d300408d-9475-457d-d63a-2427403feac8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                   Pass   R-squared:                       0.758\n",
            "Model:                            OLS   Adj. R-squared:                  0.688\n",
            "Method:                 Least Squares   F-statistic:                     10.94\n",
            "Date:                Sat, 14 Jun 2025   Prob (F-statistic):            0.00701\n",
            "Time:                        03:24:54   Log-Likelihood:               -0.17258\n",
            "No. Observations:                  10   AIC:                             6.345\n",
            "Df Residuals:                       7   BIC:                             7.253\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const            -0.3333      1.464     -0.228      0.826      -3.796       3.129\n",
            "StudyHours        0.1515      0.324      0.468      0.654      -0.615       0.918\n",
            "PrevExamScore -3.053e-16      0.054  -5.68e-15      1.000      -0.127       0.127\n",
            "==============================================================================\n",
            "Omnibus:                        0.086   Durbin-Watson:                   1.491\n",
            "Prob(Omnibus):                  0.958   Jarque-Bera (JB):                0.311\n",
            "Skew:                           0.000   Prob(JB):                        0.856\n",
            "Kurtosis:                       2.136   Cond. No.                     1.01e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.01e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=10 observations were given.\n",
            "  return hypotest_fun_in(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the model summary included p-values for each feature. The p-value indicates the statistical significance of each feature:\n",
        "\n",
        "    A p-value less than 0.05 typically suggests the feature is statistically significant.\n",
        "\n",
        "    A p-value greater than 0.05 indicates that the feature may not contribute much to the model."
      ],
      "metadata": {
        "id": "QJn7DsPZeoiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Implement backward elimination\n",
        "\n",
        "Backward elimination progressively removes the feature with the highest p-value (greater than 0.05) and refits the model with the remaining features. The goal is to continue this process until all remaining features have a p-value below the significance threshold.\n",
        "\n",
        "Here’s how we implemented the backward elimination process:"
      ],
      "metadata": {
        "id": "yn4VlM6lep3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a significance level\n",
        "significance_level = 0.05\n",
        "\n",
        "# Perform backward elimination\n",
        "while True:\n",
        "    # Fit the model\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the highest p-value in the model\n",
        "    max_p_value = model.pvalues.max()\n",
        "\n",
        "    # Check if the highest p-value is greater than the significance level\n",
        "    if max_p_value > significance_level:\n",
        "        # Identify the feature with the highest p-value\n",
        "        feature_to_remove = model.pvalues.idxmax()\n",
        "        print(f\"Removing feature: {feature_to_remove} with p-value: {max_p_value}\")\n",
        "\n",
        "        # Drop the feature\n",
        "        X = X.drop(columns=[feature_to_remove])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Display the final model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccsMSGeGe2GA",
        "outputId": "c51a602a-0a24-465d-ec88-218422518c0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing feature: PrevExamScore with p-value: 0.9999999999999956\n",
            "Removing feature: const with p-value: 0.11419580126842216\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                   Pass   R-squared (uncentered):                   0.831\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.812\n",
            "Method:                 Least Squares   F-statistic:                              44.31\n",
            "Date:                Sat, 14 Jun 2025   Prob (F-statistic):                    9.31e-05\n",
            "Time:                        03:30:52   Log-Likelihood:                         -1.8294\n",
            "No. Observations:                  10   AIC:                                      5.659\n",
            "Df Residuals:                       9   BIC:                                      5.961\n",
            "Df Model:                           1                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "StudyHours     0.1039      0.016      6.656      0.000       0.069       0.139\n",
            "==============================================================================\n",
            "Omnibus:                        0.710   Durbin-Watson:                   1.054\n",
            "Prob(Omnibus):                  0.701   Jarque-Bera (JB):                0.557\n",
            "Skew:                          -0.000   Prob(JB):                        0.757\n",
            "Kurtosis:                       1.844   Cond. No.                         1.00\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=10 observations were given.\n",
            "  return hypotest_fun_in(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the process\n",
        "\n",
        "    We first fit the model with all features and evaluated their p-values.\n",
        "\n",
        "    The feature with the highest p-value (indicating the least statistical significance) was identified and removed from the feature matrix.\n",
        "\n",
        "    The model was refitted with the remaining features, and this process was repeated until all p-values were below the significance threshold (0.05)."
      ],
      "metadata": {
        "id": "N7f3ojhlfgI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Analyze the results\n",
        "\n",
        "After completing the backward elimination process, we examined the final model summary. The remaining features in the model should all have p-values below 0.05, indicating that they are statistically significant predictors of the target variable."
      ],
      "metadata": {
        "id": "tQ2XRlQof0u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, Studyhours was the remaining feature after backward elimination, as it had a statistically significant p-value (less than 0.05). The feature Examscore was removed because its p-value exceeded the significance threshold."
      ],
      "metadata": {
        "id": "wq5yCYI9f6zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "Backward elimination helps in simplifying ML models by removing irrelevant features, which can lead to improved performance and interpretability.\n",
        "\n",
        "After completing the backward elimination process:\n",
        "\n",
        "    You should have a model that includes only the most significant features.\n",
        "\n",
        "    The model should be less prone to overfitting, as irrelevant or weak predictors have been removed.\n",
        "\n",
        "    You can now evaluate the refined model’s performance using metrics such as R-squared or other relevant evaluation methods.\n",
        "\n",
        "By following this process, you’ve learned how to effectively implement backward elimination and refine your models using feature selection. By practicing this technique, you’ll gain deeper insights into how feature selection can improve model efficiency and performance.  "
      ],
      "metadata": {
        "id": "rU8KrlIUgGNU"
      }
    }
  ]
}